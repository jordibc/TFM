#!/usr/bin/env bash
#SBATCH --job-name Humann3        # Name for your job
#SBATCH --partition=bigmem,long,medium       # total run time limit in HH:MM:SS
#SBATCH --time=1-00:00:00          # total run time limit in HH:MM:SS
#SBATCH --mail-type=end          # send email when job ends
#SBATCH --mail-user=luciamartinfernandez99@gmail.com # this is the email you wish to be notified at
#SBATCH --mem=40GB              # Reserve X GB RAM for the job
#SBATCH -c 5
#SBATCH --array=0-116
#SBATCH --output=logs/humann3-%a.%A.out # STDOUT file, %j for job id, %N for hostname
#SBATCH --error=logs/humann3-%a.%A.err  # STDERR file


#HUMANn3: 1) identifying community species with MetaPhlAn (ChocoPhlAn database), 2) mapping reads to community pangenomes (with bowtie2), and 3) aligning unmapped reads to a protein database (UniRef90) with DIAMOND.

#Installing with pip
#conda create --name biobakery3 python=3.7
#conda activate biobakery3
#pip install humann
#pip install metaphlan
#metaphlan --install  ## install bowtie2 index

#conda activate biobakery3

#Download the databases

#humann_databases --download chocophlan full /home/sbodi/miniconda3/envs/biobakery3/lib/python3.7/site-packages/humann/data --update-config yes
#humann_databases --download uniref uniref90_diamond /home/sbodi/miniconda3/envs/biobakery3/lib/python3.7/site-packages/humann/data --update-config yes
#humann_databases --download utility_mapping full /home/sbodi/miniconda3/envs/biobakery3/lib/python3.7/site-packages/humann/data --update-config yes



#Concatenate trimmed fastq files

# Este no
#cat ../00-raw_reads/samples_id.txt | xargs -I % echo "mkdir -p concatenate_fastq_fastp; cat /home/lmartin/SRV_RyC/04-fastp/%_R1.clean_qc_pair.fastq /home/lmartin/SRV_RyC/04-fastp/%_R2.clean_qc_pair.fastq > concatenate_fastq_fastp/merge_%.fq">sample_merge_fastp.sh

# Este si
#cat ../00-raw_reads/samples_id.txt | xargs -I % echo "mkdir -p concatenate_fastq_trimmo; cat /home/lmartin/SRV_RyC/02-trimmomatic/%_R1.clean_qc_pair.fastq /home/lmartin/SRV_RyC/02-trimmomatic/%_R2.clean_qc_pair.fastq > concatenate_fastq_trimmo/merge_%.fq">sample_merge_trimmo.sh 

#Running HumanN3

names=($(cat ../00-raw_reads/samples_id.txt))

#mkdir logs

#humann --input concatenate_fastq_trimmo/merge_${names[${SLURM_ARRAY_TASK_ID}]}.fq --nucleotide-database /home/sbodi/miniconda3/envs/biobakery3/lib/python3.7/site-packages/humann/data/chocophlan --protein-database /home/sbodi/miniconda3/envs/biobakery3/lib/python3.7/site-packages/humann/data/uniref --output hmn3_output/ --threads 10

#humann --input concatenate_fastq_trimmo/merge_${names[${SLURM_ARRAY_TASK_ID}]}.fq --nucleotide-database /home/lmartin/miniconda3/envs/ShotGun/lib/python3.7/site-packages/humann/data/chocophlan --protein-database /home/lmartin/miniconda3/envs/ShotGun/lib/python3.7/site-packages/humann/data/uniref --output hmn3_output/ --metaphlan-options "--index mpa_v31_CHOCOPhlAn_201901 --bowtie2db /home/lmartin/miniconda3/envs/ShotGun/lib/python3.7/site-packages/metaphlan/metaphlan_databases/mpa_v31_CHOCOPhlAn_201901" --threads 10

#humann --input concatenate_fastq_trimmo/merge_${names[${SLURM_ARRAY_TASK_ID}]}.fq --nucleotide-database /home/lmartin/miniconda3/envs/ShotGun/lib/python3.7/site-packages/humann/data/chocophlan --protein-database /home/lmartin/miniconda3/envs/ShotGun/lib/python3.7/site-packages/humann/data/uniref --output hmn3_output/

#humann --input concatenate_fastq_trimmo/merge_${names[${SLURM_ARRAY_TASK_ID}]}.fq --nucleotide-database /home/lmartin/miniconda3/envs/ShotGun/lib/python3.7/site-packages/humann/data/chocophlan --protein-database /home/lmartin/miniconda3/envs/ShotGun/lib/python3.7/site-packages/humann/data/uniref --pathways unipathway --output hmn3_output_unipathway --threads 5

# --threads N, where N is number of CPUs


#humann_join_tables -i hmn3_output -o hmn3_genefamilies.tsv --file_name genefamilies

#humann_join_tables -i hmn3_output -o hmn3_pathcoverage.tsv --file_name pathcoverage

#humann_join_tables -i hmn3_output -o hmn3_pathabundance.tsv --file_name pathabundance_relab

# The same would not be true if we were using full-scale HUMAnN databases, which recruit the vast majority of reads from metagenomes of the human microbiome. Note also that, in a real-world application, it would be particularly important to normalize our merged file to adjust for differences in sequencing depth across the samples.


#humann_renorm_table -i hmn3_genefamilies.tsv -o hmn3_genefamilies-cpm.tsv --units cpm

# Este no
#humann --input concatenate_fastq_fastp/merge_${names[${SLURM_ARRAY_TASK_ID}]}.fq --nucleotide-database /home/sbodi/miniconda3/envs/biobakery3/lib/python3.7/site-packages/humann/data/chocophlan --protein-database /home/sbodi/miniconda3/envs/biobakery3/lib/python3.7/site-packages/humann/data/uniref --output hmn3_output_fastp/

#humann_test --run-functional-tests-end-to-end

#humann_renorm_table -i hmn3_output/merge_${names[${SLURM_ARRAY_TASK_ID}]}_genefamilies.tsv --units relab -o renorm_output/merge_${names[${SLURM_ARRAY_TASK_ID}]}_renorm.tsv 
 
#humann_regroup_table -i renorm_output/merge_${names[${SLURM_ARRAY_TASK_ID}]}_renorm.tsv --custom /home/sbodi/miniconda3/envs/biobakery3/lib/python3.7/site-packages/humann/data/utility_mapping/map_ko_uniref90.txt.gz -o ko_output/merge_${names[${SLURM_ARRAY_TASK_ID}]}_ko.tsv  

humann_regroup_table -i hmn3_output/merge_${names[${SLURM_ARRAY_TASK_ID}]}_genefamilies.tsv --custom /home/sbodi/miniconda3/envs/biobakery3/lib/python3.7/site-packages/humann/data/utility_mapping/map_ko_uniref90.txt.gz -o ko_output_rpkm/merge_${names[${SLURM_ARRAY_TASK_ID}]}_ko.tsv  

#humann_regroup_table -i renorm_output/merge_${names[${SLURM_ARRAY_TASK_ID}]}_renorm.tsv --custom /home/sbodi/miniconda3/envs/biobakery3/lib/python3.7/site-packages/humann/data/utility_mapping/map_eggnog_uniref90.txt.gz -o cog_output/merge_${names[${SLURM_ARRAY_TASK_ID}]}_cog.tsv


humann_regroup_table -i hmn3_output/merge_${names[${SLURM_ARRAY_TASK_ID}]}_genefamilies.tsv --custom /home/sbodi/miniconda3/envs/biobakery3/lib/python3.7/site-packages/humann/data/utility_mapping/map_eggnog_uniref90.txt.gz -o cog_output_rpkm/merge_${names[${SLURM_ARRAY_TASK_ID}]}_cog.tsv
